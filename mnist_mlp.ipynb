{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPUQzlKqceDMBEmAGzmZFr2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Data load, transform"],"metadata":{"id":"9QQsHPMjBSlZ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":519},"id":"5EmUpNztrdA-","executionInfo":{"status":"ok","timestamp":1710255748117,"user_tz":-540,"elapsed":62591,"user":{"displayName":"김경문","userId":"17404742376652268803"}},"outputId":"d89c7c96-61ff-4a06-a060-5688c256d403"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n","  warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["(70000, 784)\n","(70000,)\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","\n","from sklearn.datasets import fetch_openml\n","from sklearn.preprocessing import OneHotEncoder\n","\n","mnist = fetch_openml('mnist_784')\n","\n","X = mnist.data.astype('float32')\n","y = mnist.target.astype('int')\n","\n","X /= 255.\n","\n","# test image\n","a = X.iloc[0]\n","\n","current_image = np.array(a).reshape(28, 28) * 255\n","plt.gray()\n","plt.imshow(current_image, interpolation='nearest')\n","plt.show()\n","\n","print(X.shape)\n","print(y.shape)"]},{"cell_type":"code","source":["# copy data\n","x_copy = X.copy()\n","y_copy = y.copy()\n","\n","# one hot encode\n","encoder = OneHotEncoder(sparse=False, categories='auto')\n","y_onehot = encoder.fit_transform(y.values.reshape(-1, 1))\n","\n","print(y_onehot.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_aGMPaN5uKQ0","executionInfo":{"status":"ok","timestamp":1710255748117,"user_tz":-540,"elapsed":7,"user":{"displayName":"김경문","userId":"17404742376652268803"}},"outputId":"26a48aab-8670-46ba-d97d-d494d2db7cd2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(70000, 10)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["### Activation function"],"metadata":{"id":"M-MFWlEqBPB0"}},{"cell_type":"code","source":["class Relu:\n","  def __init__(self, x):\n","    self.z = x\n","\n","\n","  def active(self):\n","    a = np.maximum(self.z, 0)\n","    return a\n","\n","\n","  def grd(self):\n","    grd = np.where(self.z > 0, 1, 0)\n","    return grd\n","\n","\n","\n","class LeakyRelu:\n","  def __init__(self, x):\n","    self.z = x\n","\n","\n","  def active(self):\n","    a = np.maximum(self.z, -0.01 * self.z)\n","    return a\n","\n","\n","  def grd(self):\n","    grd = np.where(self.z > 0, 1, 0.01)\n","    return grd\n","\n","\n","\n","class Softmax:\n","  def __init__(self, x):\n","    self.z = x\n","\n","\n","  def active(self):\n","    large = np.max(self.z, axis=0, keepdims=True)\n","    return np.exp(self.z - large) / np.sum(np.exp(self.z - large), axis=0, keepdims=True)\n","\n","\n","\n","class CrossEntropy:\n","  def __init__(self, pred, y):\n","    self.pred = pred\n","    self.y = y\n","\n","\n","  def loss(self):\n","    epsilon = 1e-9\n","    cross_entropy = -np.sum(self.y * np.log(self.pred + epsilon), axis=0, keepdims=True)\n","    return cross_entropy\n"],"metadata":{"id":"dyhGdTht1Vd-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Layer"],"metadata":{"id":"xYsPyBikljQ_"}},{"cell_type":"code","source":["class HiddenLayer:\n","  def __init__(self, w, b):\n","    self.w = w\n","    self.b = b\n","\n","\n","  def forward(self, x):\n","    self.x = x\n","\n","    self.z = self.w.T.dot(self.x) + self.b\n","    a = LeakyRelu(self.z).active()\n","\n","    return a\n","\n","\n","  def backward(self, get_grd, lr):\n","    dz = LeakyRelu(self.z).grd() * get_grd\n","\n","    db = np.sum(dz)\n","    dw = self.x.dot(dz.T)\n","\n","    # gradient clipping -> l2 norm\n","    norm_dw = np.linalg.norm(dw)\n","    norm_db = np.linalg.norm(db)\n","\n","    threshold = 5\n","\n","    if abs(norm_dw) >= threshold:\n","      dw = (threshold / abs(norm_dw)) * dw\n","\n","    if abs(norm_db) >= threshold:\n","      db = (threshold / abs(norm_db)) * db\n","\n","    pass_grd = self.w.dot(dz)\n","\n","    return [dw, db, pass_grd]\n","\n","\n","\n","class OutLayer:\n","  def __init__(self, w, b):\n","    self.w = w\n","    self.b = b\n","\n","\n","  def forward(self, x):\n","    self.x = x\n","    z = self.w.T.dot(self.x) + self.b\n","    a = Softmax(z).active()\n","\n","    return a\n","\n","\n","  def backward(self, get_grd, lr):\n","    dz = get_grd\n","\n","    db = np.sum(dz)\n","    dw = self.x.dot(dz.T)\n","\n","    # gradient clipping -> l2 norm\n","    norm_dw = np.linalg.norm(dw)\n","    norm_db = np.linalg.norm(db)\n","\n","    threshold = 5\n","\n","    if abs(norm_dw) >= threshold:\n","      dw = (threshold / abs(norm_dw)) * dw\n","\n","    if abs(norm_db) >= threshold:\n","      db = (threshold / abs(norm_db)) * db\n","\n","    pass_grd = self.w.dot(dz)\n","\n","    return [dw, db, pass_grd]\n"],"metadata":{"id":"RNMKC5tOlk6u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Network"],"metadata":{"id":"djwaYMsMD2VD"}},{"cell_type":"code","source":["class Network:\n","  def __init__(self, layer_num, layer_dim):\n","    self.layer_num = layer_num\n","    self.layer_dim = layer_dim\n","\n","    self.w_v = []\n","    self.b_v = []\n","\n","    network_dim = [784] + self.layer_dim + [10]\n","\n","    for idx in range(len(network_dim) - 1):\n","      w = np.random.randn(network_dim[idx], network_dim[idx + 1])\n","      b = np.zeros((network_dim[idx + 1], 1))\n","\n","      self.w_v.append(w); self.b_v.append(b)\n","\n","\n","  def forward(self, x, y):\n","    self.layer_v = []\n","    self.x = x\n","    self.y = y\n","    self.pred = None\n","\n","    layers = len(self.w_v)\n","    into = x\n","\n","    for idx in range(layers):\n","      if idx == layers - 1:\n","        out = OutLayer(self.w_v[idx], self.b_v[idx])\n","        self.layer_v.append(out)\n","        self.pred = out.forward(into)\n","\n","      else:\n","        hidden = HiddenLayer(self.w_v[idx], self.b_v[idx])\n","        self.layer_v.append(hidden)\n","        into = hidden.forward(into)\n","\n","    loss = CrossEntropy(self.pred, self.y).loss()\n","    acc = self.accuracy(self.pred, self.y)\n","\n","    return [np.mean(loss), acc]\n","\n","\n","  def backward(self, lr):\n","    grd_pass = self.pred - self.y\n","    layers = self.layer_v\n","\n","    for idx in reversed(range(len(layers))):\n","      dw, db, grd_pass = layers[idx].backward(grd_pass, lr)\n","\n","      # weight, bias update here\n","      self.update(idx, dw, db, lr)\n","\n","\n","  def update(self, idx, dw, db, lr):\n","    self.w_v[idx] = self.w_v[idx] - lr * dw\n","    self.b_v[idx] = self.b_v[idx] - lr * db\n","\n","\n","  def accuracy(self, pred, y):\n","    pred = np.argmax(pred, axis=0)\n","    y = np.argmax(y, axis=0)\n","\n","    return np.sum(pred == y) / len(y)\n","\n"],"metadata":{"id":"TXhG-YiDD40a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data split : train dev test"],"metadata":{"id":"AaeBLof5D_4m"}},{"cell_type":"code","source":["data = np.array(x_copy).T\n","label = np.array(y_onehot).T\n","\n","train_x = data[:, :50000]\n","train_y = label[:, :50000]\n","\n","dev_x = data[:, 50000:60000]\n","dev_y = label[:, 50000:60000]\n","\n","test_x = data[:, 60000:]\n","test_y = label[:, 60000:]\n","\n","print(\"train shape x, y\")\n","print(train_x.shape)\n","print(train_y.shape)\n","\n","print(\"dev shape x, y\")\n","print(dev_x.shape)\n","print(dev_y.shape)\n","\n","print(\"test shape x, y\")\n","print(test_x.shape)\n","print(test_y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lK483sxeR2xk","executionInfo":{"status":"ok","timestamp":1710255748118,"user_tz":-540,"elapsed":5,"user":{"displayName":"김경문","userId":"17404742376652268803"}},"outputId":"4170e115-114c-4326-cb4e-5807e788c1a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train shape x, y\n","(784, 50000)\n","(10, 50000)\n","dev shape x, y\n","(784, 10000)\n","(10, 10000)\n","test shape x, y\n","(784, 10000)\n","(10, 10000)\n"]}]},{"cell_type":"code","source":["test_y = train_y[]"],"metadata":{"id":"vw2-HCJ_jiz_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Mini batch *learning*"],"metadata":{"id":"InPmYy_UFsbd"}},{"cell_type":"code","source":["# bact_size, epoch 설정\n","batch_size = 32\n","epoch = 100\n","\n","# para -> 은닉층 레이어 수, 은닉층 레이어 차원 설정 : 리스트화\n","network = Network(1, [10])\n","\n","data_size = train_x.shape[1]\n","iteration = data_size // batch_size\n","\n","\n","for iter in range(epoch):\n","  # Mini batch\n","  for idx in range(iteration + 1):\n","    mini_batch_x = train_x[:, idx * batch_size : (idx + 1) * batch_size]\n","    mini_batch_y = train_y[:, idx * batch_size : (idx + 1) * batch_size]\n","\n","    network.forward(mini_batch_x, mini_batch_y)\n","    network.backward(0.01)\n","\n","  train_loss, train_acc = network.forward(train_x, train_y)\n","  val_loss, val_acc = network.forward(dev_x, dev_y)\n","\n","  print(f\"epoch : {iter + 1} / {epoch}, train acc : {train_acc}, val acc : {val_acc}\")\n","  print(train_acc - val_acc)\n","\n","  # 0.01 이상일 때 학습 멈춤\n","  if train_acc - val_acc > 0.01:\n","    print(\"Early stopping\")\n","    print(\"#\" * 30)\n","    break\n","\n","# 최종 모델 평가\n","test_loss, test_acc = network.forward(test_x, test_y)\n","print(f\"model acc : {test_acc}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5FNYPzNUYqRZ","executionInfo":{"status":"ok","timestamp":1710256683457,"user_tz":-540,"elapsed":117415,"user":{"displayName":"김경문","userId":"17404742376652268803"}},"outputId":"0073a679-2200-49b6-9f78-c7806b8f2b0d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch : 1 / 100, train acc : 0.60906, val acc : 0.6202\n","-0.011139999999999928\n","epoch : 2 / 100, train acc : 0.7535, val acc : 0.7695\n","-0.016000000000000014\n","epoch : 3 / 100, train acc : 0.80656, val acc : 0.8226\n","-0.016039999999999943\n","epoch : 4 / 100, train acc : 0.8254, val acc : 0.8399\n","-0.014499999999999957\n","epoch : 5 / 100, train acc : 0.83256, val acc : 0.8462\n","-0.013639999999999985\n","epoch : 6 / 100, train acc : 0.83934, val acc : 0.8499\n","-0.010560000000000014\n","epoch : 7 / 100, train acc : 0.84802, val acc : 0.8587\n","-0.010680000000000023\n","epoch : 8 / 100, train acc : 0.85694, val acc : 0.8668\n","-0.00985999999999998\n","epoch : 9 / 100, train acc : 0.85522, val acc : 0.8643\n","-0.009079999999999977\n","epoch : 10 / 100, train acc : 0.86202, val acc : 0.8687\n","-0.006680000000000019\n","epoch : 11 / 100, train acc : 0.85606, val acc : 0.8606\n","-0.0045399999999999885\n","epoch : 12 / 100, train acc : 0.8557, val acc : 0.862\n","-0.006299999999999972\n","epoch : 13 / 100, train acc : 0.85808, val acc : 0.8632\n","-0.005120000000000013\n","epoch : 14 / 100, train acc : 0.8601, val acc : 0.8647\n","-0.0046000000000000485\n","epoch : 15 / 100, train acc : 0.86362, val acc : 0.8694\n","-0.005779999999999896\n","epoch : 16 / 100, train acc : 0.87252, val acc : 0.8782\n","-0.005680000000000018\n","epoch : 17 / 100, train acc : 0.87762, val acc : 0.8824\n","-0.0047800000000000065\n","epoch : 18 / 100, train acc : 0.8852, val acc : 0.8896\n","-0.0043999999999999595\n","epoch : 19 / 100, train acc : 0.89078, val acc : 0.8965\n","-0.005719999999999947\n","epoch : 20 / 100, train acc : 0.89746, val acc : 0.9029\n","-0.00544\n","epoch : 21 / 100, train acc : 0.90222, val acc : 0.9072\n","-0.0049799999999999844\n","epoch : 22 / 100, train acc : 0.90526, val acc : 0.9093\n","-0.0040400000000000436\n","epoch : 23 / 100, train acc : 0.90844, val acc : 0.9121\n","-0.0036599999999999966\n","epoch : 24 / 100, train acc : 0.91142, val acc : 0.9143\n","-0.0028799999999999937\n","epoch : 25 / 100, train acc : 0.91304, val acc : 0.9168\n","-0.0037599999999999856\n","epoch : 26 / 100, train acc : 0.9144, val acc : 0.9173\n","-0.0029000000000000137\n","epoch : 27 / 100, train acc : 0.91548, val acc : 0.9181\n","-0.0026200000000000667\n","epoch : 28 / 100, train acc : 0.91688, val acc : 0.9187\n","-0.0018199999999999328\n","epoch : 29 / 100, train acc : 0.91768, val acc : 0.9188\n","-0.0011199999999998989\n","epoch : 30 / 100, train acc : 0.91828, val acc : 0.9191\n","-0.0008200000000000429\n","epoch : 31 / 100, train acc : 0.91932, val acc : 0.9198\n","-0.00047999999999992493\n","epoch : 32 / 100, train acc : 0.92058, val acc : 0.9205\n","7.999999999996898e-05\n","epoch : 33 / 100, train acc : 0.92154, val acc : 0.9214\n","0.000140000000000029\n","epoch : 34 / 100, train acc : 0.92228, val acc : 0.9223\n","-2.0000000000020002e-05\n","epoch : 35 / 100, train acc : 0.92248, val acc : 0.9226\n","-0.00012000000000000899\n","epoch : 36 / 100, train acc : 0.92302, val acc : 0.9232\n","-0.000180000000000069\n","epoch : 37 / 100, train acc : 0.92336, val acc : 0.9233\n","5.999999999994898e-05\n","epoch : 38 / 100, train acc : 0.92378, val acc : 0.9238\n","-1.999999999990898e-05\n","epoch : 39 / 100, train acc : 0.92424, val acc : 0.9243\n","-6.0000000000060005e-05\n","epoch : 40 / 100, train acc : 0.92452, val acc : 0.9238\n","0.0007200000000000539\n","epoch : 41 / 100, train acc : 0.92496, val acc : 0.9243\n","0.0006599999999999939\n","epoch : 42 / 100, train acc : 0.92506, val acc : 0.9245\n","0.0005600000000000049\n","epoch : 43 / 100, train acc : 0.9255, val acc : 0.9246\n","0.0009000000000000119\n","epoch : 44 / 100, train acc : 0.92558, val acc : 0.9246\n","0.0009799999999999809\n","epoch : 45 / 100, train acc : 0.92564, val acc : 0.9246\n","0.001040000000000041\n","epoch : 46 / 100, train acc : 0.92588, val acc : 0.9247\n","0.0011800000000000699\n","epoch : 47 / 100, train acc : 0.92602, val acc : 0.9245\n","0.0015199999999999658\n","epoch : 48 / 100, train acc : 0.92596, val acc : 0.9242\n","0.0017599999999999838\n","epoch : 49 / 100, train acc : 0.92592, val acc : 0.9235\n","0.0024199999999999777\n","epoch : 50 / 100, train acc : 0.92572, val acc : 0.9234\n","0.0023199999999999887\n","epoch : 51 / 100, train acc : 0.9258, val acc : 0.9235\n","0.0022999999999999687\n","epoch : 52 / 100, train acc : 0.92568, val acc : 0.9233\n","0.0023799999999999377\n","epoch : 53 / 100, train acc : 0.92576, val acc : 0.9231\n","0.0026599999999999957\n","epoch : 54 / 100, train acc : 0.92582, val acc : 0.9234\n","0.0024199999999999777\n","epoch : 55 / 100, train acc : 0.92614, val acc : 0.9232\n","0.0029399999999999427\n","epoch : 56 / 100, train acc : 0.92608, val acc : 0.9228\n","0.0032800000000000606\n","epoch : 57 / 100, train acc : 0.92606, val acc : 0.923\n","0.0030599999999999516\n","epoch : 58 / 100, train acc : 0.92584, val acc : 0.9232\n","0.0026399999999999757\n","epoch : 59 / 100, train acc : 0.92594, val acc : 0.9233\n","0.0026399999999999757\n","epoch : 60 / 100, train acc : 0.92586, val acc : 0.923\n","0.0028599999999999737\n","epoch : 61 / 100, train acc : 0.92578, val acc : 0.923\n","0.0027800000000000047\n","epoch : 62 / 100, train acc : 0.92576, val acc : 0.9228\n","0.0029600000000000737\n","epoch : 63 / 100, train acc : 0.92592, val acc : 0.9225\n","0.0034199999999999786\n","epoch : 64 / 100, train acc : 0.9261, val acc : 0.9225\n","0.0036000000000000476\n","epoch : 65 / 100, train acc : 0.92608, val acc : 0.9228\n","0.0032800000000000606\n","epoch : 66 / 100, train acc : 0.92606, val acc : 0.9228\n","0.0032600000000000406\n","epoch : 67 / 100, train acc : 0.9262, val acc : 0.9222\n","0.0040000000000000036\n","epoch : 68 / 100, train acc : 0.92636, val acc : 0.9226\n","0.0037599999999999856\n","epoch : 69 / 100, train acc : 0.92632, val acc : 0.9228\n","0.0035200000000000786\n","epoch : 70 / 100, train acc : 0.9264, val acc : 0.9224\n","0.0040000000000000036\n","epoch : 71 / 100, train acc : 0.92638, val acc : 0.9226\n","0.0037800000000000056\n","epoch : 72 / 100, train acc : 0.92618, val acc : 0.9224\n","0.0037800000000000056\n","epoch : 73 / 100, train acc : 0.92616, val acc : 0.9221\n","0.0040599999999999525\n","epoch : 74 / 100, train acc : 0.9261, val acc : 0.9217\n","0.0044000000000000705\n","epoch : 75 / 100, train acc : 0.92596, val acc : 0.9216\n","0.0043600000000000305\n","epoch : 76 / 100, train acc : 0.92586, val acc : 0.9214\n","0.0044600000000000195\n","epoch : 77 / 100, train acc : 0.92586, val acc : 0.9211\n","0.0047599999999999865\n","epoch : 78 / 100, train acc : 0.92564, val acc : 0.921\n","0.0046399999999999775\n","epoch : 79 / 100, train acc : 0.92538, val acc : 0.9206\n","0.0047800000000000065\n","epoch : 80 / 100, train acc : 0.92542, val acc : 0.9202\n","0.005220000000000002\n","epoch : 81 / 100, train acc : 0.9254, val acc : 0.9202\n","0.005199999999999982\n","epoch : 82 / 100, train acc : 0.92544, val acc : 0.9202\n","0.005240000000000022\n","epoch : 83 / 100, train acc : 0.92564, val acc : 0.9202\n","0.00544\n","epoch : 84 / 100, train acc : 0.92578, val acc : 0.9203\n","0.00548000000000004\n","epoch : 85 / 100, train acc : 0.92574, val acc : 0.9201\n","0.005639999999999978\n","epoch : 86 / 100, train acc : 0.92602, val acc : 0.9201\n","0.005919999999999925\n","epoch : 87 / 100, train acc : 0.9262, val acc : 0.9199\n","0.006299999999999972\n","epoch : 88 / 100, train acc : 0.92622, val acc : 0.9201\n","0.006120000000000014\n","epoch : 89 / 100, train acc : 0.92606, val acc : 0.9202\n","0.005859999999999976\n","epoch : 90 / 100, train acc : 0.92594, val acc : 0.9199\n","0.006039999999999934\n","epoch : 91 / 100, train acc : 0.92592, val acc : 0.9197\n","0.006220000000000003\n","epoch : 92 / 100, train acc : 0.92618, val acc : 0.9193\n","0.006879999999999997\n","epoch : 93 / 100, train acc : 0.92632, val acc : 0.9196\n","0.006720000000000059\n","epoch : 94 / 100, train acc : 0.92648, val acc : 0.9201\n","0.006379999999999941\n","epoch : 95 / 100, train acc : 0.92698, val acc : 0.9204\n","0.00658000000000003\n","epoch : 96 / 100, train acc : 0.92714, val acc : 0.9208\n","0.006340000000000012\n","epoch : 97 / 100, train acc : 0.92722, val acc : 0.9203\n","0.006920000000000037\n","epoch : 98 / 100, train acc : 0.92742, val acc : 0.9208\n","0.00662000000000007\n","epoch : 99 / 100, train acc : 0.92732, val acc : 0.9205\n","0.006820000000000048\n","epoch : 100 / 100, train acc : 0.92742, val acc : 0.9206\n","0.006820000000000048\n","model acc : 0.9129\n"]}]}]}